{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from step_4.utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = ''\n",
    "DATASET = 'coco'\n",
    "MODEL = 'FastSAM'\n",
    "TARGET = 'SAM'\n",
    "SPARSITY = 0\n",
    "ROOT, N, CLASSES = get_dataset_info(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Metrics\n",
    "\n",
    "def calculate_metrics(target, pred, eps=1e-5, verbose=False):\n",
    "\n",
    "    if verbose:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(target)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(pred)\n",
    "        plt.show()\n",
    "\n",
    "    output = np.reshape(pred, -1)\n",
    "    target = np.reshape(target, -1)\n",
    "\n",
    "    tp = np.sum(output * target)  # TP (Intersection)\n",
    "    un = np.sum(output + target)  # Union\n",
    "    fp = np.sum(output * (~target))  # FP\n",
    "    fn = np.sum((~output) * target)  # FN\n",
    "    tn = np.sum((~output) * (~target))  # TN\n",
    "\n",
    "    iou = (tp + eps) / (un + eps)\n",
    "    pixel_acc = (tp + tn + eps) / (tp + tn + fp + fn + eps)\n",
    "    dice = (2 * tp + eps) / (2 * tp + fp + fn + eps)\n",
    "    precision = (tp + eps) / (tp + fp + eps)\n",
    "    recall = (tp + eps) / (tp + fn + eps)\n",
    "    specificity = (tn + eps) / (tn + fp + eps)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"IoU: {iou:.4f}, Pixel Acc: {pixel_acc:.4f}, Dice: {dice:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return iou, pixel_acc, dice, precision, specificity, recall\n",
    "\n",
    "def get_analytics(target_df, pred_df):\n",
    "    metrics = {k: [] for k in ['name', 'prompt', 'class', 't_class', 's_class', 'score', 'score_diff', 'mask_size', \n",
    "                               'mask_size_diff', 'iou', 'pixel_acc', 'dice', 'precision', 'recall', 'specificity']}\n",
    "    for i in range(len(target_df)):\n",
    "        target = target_df.loc[i]\n",
    "        pred = pred_df.loc[i]\n",
    "\n",
    "        iou, pixel_acc, dice, precision, specificity, recall = calculate_metrics(target['mask'], pred['mask'])\n",
    "        \n",
    "        metrics['name'].append(target['name'])\n",
    "        metrics['prompt'].append(target['prompt'])\n",
    "        metrics['class'].append(target['class'])\n",
    "        metrics['t_class'].append(target['s_class'])\n",
    "        metrics['s_class'].append(pred['s_class'])\n",
    "        metrics['score'].append(pred['score'])\n",
    "        metrics['score_diff'].append((pred['score'] - target['score']) / (target['score'] + 1e-5))\n",
    "        p_size = np.mean(pred['mask'].astype('float'))\n",
    "        t_size = np.mean(target['mask'].astype('float'))\n",
    "        metrics['mask_size'].append(p_size)\n",
    "        metrics['mask_size_diff'].append((p_size - t_size) / (t_size + 1e-3))\n",
    "        metrics['iou'].append(iou)\n",
    "        metrics['pixel_acc'].append(pixel_acc)\n",
    "        metrics['dice'].append(dice)\n",
    "        metrics['precision'].append(precision)\n",
    "        metrics['recall'].append(recall)\n",
    "        metrics['specificity'].append(specificity)\n",
    "    \n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "# Visualization\n",
    "\n",
    "def get_labels(name):\n",
    "    if isinstance(name, list):\n",
    "        return [get_labels(n) for n in name]\n",
    "    else: \n",
    "        return CLASSES[name].title()\n",
    "\n",
    "def get_image(name):\n",
    "    if DATASET == 'coco':\n",
    "        image_path = ROOT.joinpath(f'{str(name).zfill(12)}.jpg')\n",
    "    else:\n",
    "        image_path = ROOT.joinpath(f\"{name.split('_')[0]}/{name}\")\n",
    "    return np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "\n",
    "def show_entry(row, target_df, pred_df):\n",
    "    image = get_image(row['name'])\n",
    "    target_mask = target_df[target_df['name']==row['name']]['mask'].values[0]\n",
    "    pred_mask = pred_df[pred_df['name']==row['name']]['mask'].values[0]\n",
    "    show_points_and_masks_on_image(image, [pred_mask, target_mask], [row['prompt']])\n",
    "    print(f'ID: {row[\"name\"]}, PromptClass: {get_labels(row[\"class\"])}, TargetClass: {get_labels(row[\"t_class\"])}, PredClass: {get_labels(row[\"s_class\"])},') \n",
    "    print(f'ScoreDiff: {row[\"score_diff\"]:.4f}, MaskSizeDiff: {row[\"mask_size_diff\"]:.4f}, IoU: {row[\"iou\"]:.4f}')\n",
    "    \n",
    "def show_samples(pie_df, target_df, pred_df, n=5):\n",
    "    print('Legend: Target -> Orange, Prediction -> Blue')\n",
    "    pie_df.iloc[:n].apply(lambda x: show_entry(x, target_df, pred_df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.read_pickle(f\"results/{EXPERIMENT}{DATASET}_prompts.pkl\")\n",
    "df_0 = pd.read_pickle(f\"results/{EXPERIMENT}{DATASET}_{TARGET}_0.pkl\")\n",
    "df_s = pd.read_pickle(f\"results/{EXPERIMENT}{DATASET}_{MODEL}_{SPARSITY}.pkl\")\n",
    "df_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0s = get_analytics(df_0, df_s)\n",
    "df_0s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_0s['mask_size_diff'])\n",
    "plt.semilogy()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_size = df_0s.nsmallest(25, ['mask_size_diff'])\n",
    "max_size = df_0s.nlargest(25, ['mask_size_diff'])\n",
    "min_score = df_0s.nsmallest(25, ['score_diff']) # not very useful\n",
    "max_score = df_0s.nlargest(25, ['score_diff']) # not very useful\n",
    "min_iou = df_0s.nsmallest(25, ['iou'])\n",
    "max_iou = df_0s.nlargest(25, ['iou'])\n",
    "min_size.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iou.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(min_score, df_0, df_s, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0s.hist(column='mask_size')\n",
    "plt.semilogy()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
