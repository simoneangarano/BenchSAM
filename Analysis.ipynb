{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "'EXPERIMENT': 'filter_',\n",
    "'DATASET': 'coco',\n",
    "'MODEL': 'SAM',\n",
    "'TARGET': 'SAM',\n",
    "'SPARSITY': 80,\n",
    "'CLASS': 0,\n",
    "'MODE': '',\n",
    "'METRIC': 'iou'\n",
    "}\n",
    "cfg['ROOT'], cfg['N'], cfg['CLASSES'], cfg['SUPERCLASSES'], cfg['SUP_N'] = get_dataset_info(cfg['DATASET'])\n",
    "cfg['SUP_L'] = {v: k for k, v in cfg['SUP_N'].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.read_pickle(f\"results/{cfg['EXPERIMENT']}{cfg['DATASET']}_prompts.pkl\")\n",
    "df_0 = pd.read_pickle(f\"results/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['TARGET']}_0.pkl\")\n",
    "df_s = pd.read_pickle(f\"results/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['MODEL']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Image Shape\n",
    "if 'shape' not in df_p.keys():\n",
    "    df_p = add_image_shape(df_p)\n",
    "    df_p.head()\n",
    "    df_p.to_pickle(f\"results/{cfg['EXPERIMENT']}{cfg['DATASET']}_prompts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0s = get_analytics(df_0, df_s, df_p)\n",
    "df_0s.to_pickle(f\"analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['MODEL']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\")\n",
    "df_0s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0s.hist(column='class', bins=cfg['N'])\n",
    "plt.semilogy()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0s.hist(column='iou')\n",
    "plt.semilogy()\n",
    "plt.show()\n",
    "df_0s['iou'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Save Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(10, 100, 10):\n",
    "    cfg['SPARSITY'] = s\n",
    "    save_analytics(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Summary\n",
    "summary = []\n",
    "for m in MODES:\n",
    "    cfg['MODE'] = m\n",
    "    for s in SPARSITIES:\n",
    "        cfg['SPARSITY'] = s\n",
    "        summary.append(get_summary(cfg))\n",
    "\n",
    "cfg['MODE'] = ''\n",
    "cfg['SPARSITY'] = 0\n",
    "for n in MODELS[1:]:\n",
    "    cfg['MODEL'] = n\n",
    "    summary.append(get_summary(cfg))\n",
    "summary = pd.DataFrame(summary)\n",
    "summary.to_pickle(f\"analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_summary.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms\n",
    "for t in METRICS:\n",
    "    cfg['METRIC'] = t\n",
    "    for m in MODES:\n",
    "        cfg['MODE'] = m\n",
    "        summary = []\n",
    "        for s in SPARSITIES:\n",
    "            cfg['SPARSITY'] = s\n",
    "            summary.append(get_summary(cfg))\n",
    "        summary = pd.DataFrame(summary)\n",
    "        get_hists(summary, cfg, save=False, plot=False)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.read_pickle(f\"analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_summary.pkl\")\n",
    "summary.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in METRICS:\n",
    "    cfg['METRIC'] = m\n",
    "    get_curves(summary, cfg, std=False, plot=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class-wise Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Summary\n",
    "summary = []\n",
    "for m in MODES:\n",
    "    cfg['MODE'] = m\n",
    "    for s in SPARSITIES:\n",
    "        cfg['SPARSITY'] = s\n",
    "        summary.append(get_summary(cfg, classwise=True))\n",
    "\n",
    "cfg['MODE'] = ''\n",
    "cfg['SPARSITY'] = 0\n",
    "for n in MODELS[1:]:\n",
    "    cfg['MODEL'] = n\n",
    "    summary.append(get_summary(cfg, classwise=True))\n",
    "summary = pd.concat(summary)\n",
    "summary.to_pickle(f\"analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_classwise.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = summary[summary['pruning']=='_gup']\n",
    "d = d[d['id']==50]\n",
    "d = d[d['class']!=0]\n",
    "get_hists(d, cfg, save=False, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class-wise Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.read_pickle(f\"analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_classwise.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classwise_curves(s, cfg, plot=True, save=False):\n",
    "    _, ax = plt.subplots()\n",
    "    ax.grid()\n",
    "    ax.title.set_text(cfg['METRIC'].replace('_', ' ').capitalize())\n",
    "    ax.plot(PARAMS['SAM'], SAM[cfg['METRIC']], 'o')\n",
    "     \n",
    "    for c in range(1, d.shape[1]):\n",
    "        if cfg['MODEL'] == 'SAM':\n",
    "            p = (1 - d[:,c,1]/100) * PARAMS['SAM'] \n",
    "        else:\n",
    "            p = PARAMS[d[:,c,1][0]]\n",
    "        ax.plot(p, d[:,c,2], '-o', label=f\"{cfg['SUP_L'][c]} ({d[0,c,3]})\")\n",
    "    # Shrink current axis's height by 10% on the bottom\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), fancybox=False, shadow=False, ncol=4, fontsize=9)   \n",
    "    #plt.legend(loc='best', fontsize=9, ncol=2)\n",
    "    plt.xlabel('Parameters')\n",
    "    plt.ylabel(cfg['METRIC'].replace('_', ' ').capitalize())\n",
    "    plt.axvline(PARAMS['SAM'], linestyle='--')\n",
    "    plt.axhline(SAM[cfg['METRIC']], linestyle='--')\n",
    "    plt.semilogx()\n",
    "    if save:\n",
    "        plt.savefig(f\"figures/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['METRIC']}_{cfg['MODEL']}{cfg['MODE']}_curves_classwise.pdf\", bbox_inches='tight')\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUP\n",
    "for m in METRICS:\n",
    "    cfg['METRIC'] = m\n",
    "    d = summary[summary['pruning']=='_gup'][['class', 'id', cfg['METRIC'], 'n']].values.reshape(9, 13, -1)\n",
    "    get_classwise_curves(d, cfg, plot=False, save=False)\n",
    "\n",
    "# SparseGPT\n",
    "cfg['MODE'] = '_gup'\n",
    "for m in METRICS:\n",
    "    cfg['METRIC'] = m\n",
    "    d = summary[summary['id'].str.isalpha().isnull()]\n",
    "    d = d[d['pruning']!='_gup'][['class', 'id', cfg['METRIC'], 'n']].values.reshape(9, 13, -1)\n",
    "    get_classwise_curves(d, cfg, plot=False, save=False)\n",
    "\n",
    "# FastSAM\n",
    "cfg['MODEL'] = 'FastSAM'\n",
    "for m in METRICS:\n",
    "    cfg['METRIC'] = m\n",
    "    d = summary[summary['id']==cfg['MODEL']][['class', 'id', cfg['METRIC'], 'n']].values.reshape(1, 13, -1)\n",
    "    get_classwise_curves(d, cfg, plot=False, save=False)\n",
    "\n",
    "# MobileSAM\n",
    "cfg['MODEL'] = 'MobileSAM'\n",
    "for m in METRICS:\n",
    "    cfg['METRIC'] = m\n",
    "    d = summary[summary['id']==cfg['MODEL']][['class', 'id', cfg['METRIC'], 'n']].values.reshape(1, 13, -1)\n",
    "    get_classwise_curves(d, cfg, plot=False, save=False)\n",
    "\n",
    "# Mobile&FastSAM\n",
    "cfg['MODEL'] = 'Mobile&FastSAM'\n",
    "for m in METRICS:\n",
    "    cfg['METRIC'] = m\n",
    "    d = summary[summary['id'].str.isalpha().notnull()][['class', 'id', cfg['METRIC'], 'n']].values.reshape(1, 13, -1)\n",
    "    get_classwise_curves(d, cfg, plot=False, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forgettability_curve_dissimilarity(d, cfg, plot=True, save=False):\n",
    "    fcd = np.zeros((d.shape[1], d.shape[1]))\n",
    "    for c1 in range(d.shape[1]):\n",
    "        for c2 in range(c1, d.shape[1]):\n",
    "            if c1 == c2:\n",
    "                continue\n",
    "            fcd[c1, c2] = ((d[:,c1,2] - d[:,c2,2])**2).mean()\n",
    "            fcd[c2, c1] = fcd[c1, c2]\n",
    "\n",
    "    plt.imshow(fcd, interpolation='nearest')\n",
    "    plt.xticks(range(12), [cfg['SUP_L'][i] for i in range(1, 13)], rotation=90)\n",
    "    plt.yticks(range(12), [cfg['SUP_L'][i] for i in range(1, 13)])\n",
    "    plt.title('Forgettability Curve Dissimilarity - GUP')\n",
    "    plt.colorbar()\n",
    "    if save:\n",
    "        plt.savefig(f\"figures/{cfg['EXPERIMENT']}{cfg['DATASET']}{cfg['MODE']}_fcd.pdf\", bbox_inches='tight')\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg['MODE'] == '':\n",
    "    # SparseGPT\n",
    "    d = summary[summary['id'].str.isalpha().isnull()]\n",
    "    d = d[d['pruning']!='_gup'][['class', 'id', cfg['METRIC'], 'n']].values.reshape(9, 13, -1)[:,1:,:]\n",
    "elif cfg['MODE'] == '_gup':\n",
    "    # GUP\n",
    "    d = summary[summary['pruning']=='_gup'][['class', 'id', cfg['METRIC']]].values.reshape(9, 13, -1)[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_forgettability_curve_dissimilarity(d, cfg, plot=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(f\"analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['MODEL']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column='iou')\n",
    "plt.semilogy()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_superclass(df, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['iou', 'precision', 'recall', 'mask_size', 'mask_size_diff', 'superclass']\n",
    "data = df[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'] = get_clusters(data, cfg, plot=False, save=False)\n",
    "df.to_pickle(f\"analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['MODEL']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "m = 'recall'\n",
    "df[df['cluster']==c].hist(column=m)\n",
    "plt.show()\n",
    "df[df['cluster']==c][m].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastSAM, SAM\n",
    "- 0 is the best cluster\n",
    "- 1 has low recall, high precision, smaller masks, smaller mask size difference, low iou -> tends to predict smaller (empty) masks\n",
    "- 2 has low precision, high recall, bigger masks, bigger mask size difference, lower iou -> tends to predict bigger (whole image) masks\n",
    "\n",
    "MobileSAM\n",
    "- 2 is the best cluster\n",
    "- 1 has low recall, high precision, smaller masks, smaller mask size difference, low iou -> tends to predict smaller (empty) masks\n",
    "- 0 has low precision, high recall, bigger masks, bigger mask size difference, lower iou -> tends to predict bigger (whole image) masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(f\"analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['MODEL']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\")\n",
    "df_0 = pd.read_pickle(f\"results/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['TARGET']}_0.pkl\")\n",
    "df_s = pd.read_pickle(f\"results/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['MODEL']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\")\n",
    "df = df[df['cluster']==C].sort_values(by='iou', ascending=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(df, df_0, df_s, cfg, prompt_zoom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_size = df_0s.nsmallest(25, ['mask_size_diff'])\n",
    "max_size = df_0s.nlargest(25, ['mask_size_diff'])\n",
    "min_score = df_0s.nsmallest(25, ['score_diff']) # not very useful\n",
    "max_score = df_0s.nlargest(25, ['score_diff']) # not very useful\n",
    "min_iou = df_0s.nsmallest(25, ['iou'])\n",
    "max_iou = df_0s.nlargest(25, ['iou'])\n",
    "min_size.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iou.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(max_size, df_0, df_s, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(f\"results/test_sa1b_SAM_0.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = df.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.array(Image.open(cfg['ROOT'].joinpath(f\"0/{e['name']}.jpg\")).convert(\"RGB\"))\n",
    "i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = e['mask']\n",
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = e['mask_origin']\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(i[o[0]:o[0]+m.shape[0], o[1]:o[1]+m.shape[1], :])\n",
    "plt.imshow(m, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_mask(mask, origin, full_shape):\n",
    "    full_mask = np.zeros(full_shape)\n",
    "    full_mask[origin[0]:origin[0]+mask.shape[0], origin[1]:origin[1]+mask.shape[1]] = mask\n",
    "    return full_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(i)\n",
    "plt.imshow(get_full_mask(m, o, i.shape[:2]), alpha=0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
