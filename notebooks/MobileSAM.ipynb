{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils.datasets import COCOSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='../Datasets/coco-2017/'\n",
    "dataType='val2017'\n",
    "annFile='{}/annotations/instances_{}.json'.format(dataDir,dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = COCOSegmentation(dataDir, 'val', crop_size=0)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True, worker_init_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, l, n = dataset[0]\n",
    "i.shape, l.shape, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, l, n) in dataloader:\n",
    "    print(i.shape, l.shape, n)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(i[0])\n",
    "plt.imshow(l[0], alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "l.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils.mobile_sam import sam_model_registry, SamPredictor\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "GPU = 3\n",
    "\n",
    "device = torch.device(f\"cuda:{GPU}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Sam:\n\tMissing key(s) in state_dict: \"prompt_encoder.point_embeddings.4.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sa58728/BenchSAM/notebooks/MobileSAM.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvita10.ece.utexas.edu/home/sa58728/BenchSAM/notebooks/MobileSAM.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvit_t\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvita10.ece.utexas.edu/home/sa58728/BenchSAM/notebooks/MobileSAM.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m sam_checkpoint \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../bin/mobile_sam.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvita10.ece.utexas.edu/home/sa58728/BenchSAM/notebooks/MobileSAM.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m sam_model_registry[model_type](checkpoint\u001b[39m=\u001b[39;49msam_checkpoint)\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvita10.ece.utexas.edu/home/sa58728/BenchSAM/notebooks/MobileSAM.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m predictor \u001b[39m=\u001b[39m SamPredictor(model)\n",
      "File \u001b[0;32m~/BenchSAM/notebooks/../utils/mobile_sam/build_sam.py:94\u001b[0m, in \u001b[0;36mbuild_sam_vit_t\u001b[0;34m(checkpoint, size_embedding)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(checkpoint, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     93\u001b[0m         state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m---> 94\u001b[0m     mobile_sam\u001b[39m.\u001b[39;49mload_state_dict(state_dict)\n\u001b[1;32m     95\u001b[0m \u001b[39mreturn\u001b[39;00m mobile_sam\n",
      "File \u001b[0;32m~/anaconda3/envs/sam/lib/python3.9/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Sam:\n\tMissing key(s) in state_dict: \"prompt_encoder.point_embeddings.4.weight\". "
     ]
    }
   ],
   "source": [
    "model_type = \"vit_t\"\n",
    "sam_checkpoint = \"../bin/mobile_sam.pt\"\n",
    "\n",
    "model = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(device).eval()\n",
    "predictor = SamPredictor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): TinyViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (seq): Sequential(\n",
       "        (0): Conv2d_BN(\n",
       "          (c): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Conv2d_BN(\n",
       "          (c): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ConvLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x MBConv(\n",
       "            (conv1): Conv2d_BN(\n",
       "              (c): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (act1): GELU(approximate='none')\n",
       "            (conv2): Conv2d_BN(\n",
       "              (c): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (act2): GELU(approximate='none')\n",
       "            (conv3): Conv2d_BN(\n",
       "              (c): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (act3): GELU(approximate='none')\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (act): GELU(approximate='none')\n",
       "          (conv1): Conv2d_BN(\n",
       "            (c): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d_BN(\n",
       "            (c): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d_BN(\n",
       "            (c): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): BasicLayer(\n",
       "        dim=128, input_resolution=(128, 128), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x TinyViTBlock(\n",
       "            dim=128, input_resolution=(128, 128), num_heads=4, window_size=7, mlp_ratio=4.0\n",
       "            (drop_path): Identity()\n",
       "            (attn): Attention(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (local_conv): Conv2d_BN(\n",
       "              (c): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (act): GELU(approximate='none')\n",
       "          (conv1): Conv2d_BN(\n",
       "            (c): Conv2d(128, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d_BN(\n",
       "            (c): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=160, bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d_BN(\n",
       "            (c): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): BasicLayer(\n",
       "        dim=160, input_resolution=(64, 64), depth=6\n",
       "        (blocks): ModuleList(\n",
       "          (0-5): 6 x TinyViTBlock(\n",
       "            dim=160, input_resolution=(64, 64), num_heads=5, window_size=14, mlp_ratio=4.0\n",
       "            (drop_path): Identity()\n",
       "            (attn): Attention(\n",
       "              (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "              (qkv): Linear(in_features=160, out_features=480, bias=True)\n",
       "              (proj): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=160, out_features=640, bias=True)\n",
       "              (fc2): Linear(in_features=640, out_features=160, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (local_conv): Conv2d_BN(\n",
       "              (c): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
       "              (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (act): GELU(approximate='none')\n",
       "          (conv1): Conv2d_BN(\n",
       "            (c): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d_BN(\n",
       "            (c): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d_BN(\n",
       "            (c): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): BasicLayer(\n",
       "        dim=320, input_resolution=(64, 64), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x TinyViTBlock(\n",
       "            dim=320, input_resolution=(64, 64), num_heads=10, window_size=7, mlp_ratio=4.0\n",
       "            (drop_path): Identity()\n",
       "            (attn): Attention(\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (qkv): Linear(in_features=320, out_features=960, bias=True)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (local_conv): Conv2d_BN(\n",
       "              (c): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_head): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=320, out_features=1000, bias=True)\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-4): 5 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'bin/distilled_mobile_sam_online.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n",
    "raw_image = np.array(Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\"))\n",
    "# plt.imshow(raw_image)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference with Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_points = np.array([[450, 600]])\n",
    "input_label = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_points = np.array([[450, 600]])\n",
    "input_label = np.array([1])\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictor.set_image(raw_image)\n",
    "    output = predictor.predict(input_points, input_label, return_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, scores, lowres = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape, scores.shape, lowres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(raw_image)\n",
    "plt.imshow(masks[0], alpha=0.5)\n",
    "plt.imshow(masks[1], alpha=0.5)\n",
    "plt.imshow(masks[2], alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(masks[0].min(), masks[0].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masks[np.argmax(scores)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CENTER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_masks(processor, model, i, input_points, device):\n",
    "    i = i[0].detach().cpu().numpy().astype(np.uint8)\n",
    "    predictor.set_image(i)\n",
    "    masks, scores, _ = predictor.predict(np.array(input_points[0]), np.array([1]))\n",
    "    return masks, scores\n",
    "    \n",
    "def get_prompt(name, label):\n",
    "\n",
    "    # Load_prompts missing\n",
    "\n",
    "    C = np.unique(label)[1:]\n",
    "    c = np.random.choice(C)\n",
    "\n",
    "    if CENTER:\n",
    "        x, y = torch.sum(torch.argwhere(label==c),0)/torch.sum(label==c).detach().cpu().numpy()\n",
    "        x, y = int(x), int(y)\n",
    "    else:\n",
    "        x_v, y_v = np.where(label == c)\n",
    "        r = random.randint(0,len(x_v))\n",
    "        x, y = x_v[r], y_v[r]\n",
    "    return [[[y,x]]], c # inverted to compensate different indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks():\n",
    "\n",
    "    name_list, mask_list, score_list, prompt_list, p_class_list = [], [], [], [], []\n",
    "    for j, (i, l, n) in enumerate(dataloader):\n",
    "\n",
    "        prompt, p_class = get_prompt(n, l[0])\n",
    "        # show_points_on_image(i[0], input_points[0])\n",
    "\n",
    "        masks, scores = get_output_masks(None, predictor, i, prompt, device)\n",
    "        # show_masks_on_image(i[0], masks, scores)  \n",
    "        name_list.append(int(n[0]))\n",
    "        mask_list.append(masks.squeeze()[scores.argmax()])\n",
    "        score_list.append(float(scores.max()))\n",
    "        prompt_list.append(prompt[0][0])\n",
    "        p_class_list.append(int(p_class))\n",
    "\n",
    "        if j > 1:\n",
    "            break\n",
    "\n",
    "    return name_list, prompt_list, p_class_list, mask_list, score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, prompt, p_class, mask, score = get_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "name[i], prompt[i], p_class[i], mask[i].shape, score[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open('../Datasets/coco-2017/val2017/' + str(name[i]).zfill(12) + '.jpg')\n",
    "plt.imshow(im)\n",
    "plt.imshow(mask[i], alpha=0.5)\n",
    "print(dataset.classes[p_class[i]])\n",
    "print(name[i])\n",
    "plt.scatter(*prompt[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'name': name, 'prompt': prompt, 'class': p_class, 'mask': mask, 'score': score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['name', 'point', 'class']].to_pickle(\"results/coco_prompts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['name', 'point', 'class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"results/cityscapes_prompts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['name']==632][['point', 'class']].values[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance(label, c=None):\n",
    "    if c is None:\n",
    "        C = np.unique(label)[1:]\n",
    "        c = np.random.choice(C)\n",
    "        return label == c, c\n",
    "    else:\n",
    "        return label == c, c\n",
    "\n",
    "def get_pred_classes(inst, label, n_classes, threshold=0.01):\n",
    "    im = torch.logical_not(inst).to(torch.uint8)\n",
    "    im[im==1] = n_classes\n",
    "    m = im + label\n",
    "    h, _ = np.histogram(m, bins=256, range=(0,255))\n",
    "    clean_h = h[:n_classes]\n",
    "    mask_tot = np.sum(clean_h)\n",
    "    classes = np.where(clean_h > threshold * mask_tot)[0]\n",
    "    return list(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test class threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label\n",
    "l = torch.zeros((224,224), dtype=torch.uint8)\n",
    "l[100:150, 50:100] = 35\n",
    "l[145:150, 95:100] = 91\n",
    "l[100:140, 160:200] = 60\n",
    "l[100:115, 50:65] = 0\n",
    "\n",
    "plt.imshow(l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted instance\n",
    "i = torch.zeros((224,224), dtype=bool)\n",
    "i[100:150, 50:100] = True\n",
    "plt.imshow(i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pred_classes(i, l, N_CLASSES, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified instance\n",
    "im = torch.logical_not(i).to(torch.uint8)\n",
    "im[im==1] = N_CLASSES\n",
    "\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.min(), im.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask (intersection)\n",
    "m = im + l\n",
    "\n",
    "plt.imshow(m)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, _ = np.histogram(m, bins=256, range=(0,255))\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_h = h[:N_CLASSES]\n",
    "clean_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_tot = np.sum(clean_h)\n",
    "mask_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(clean_h > 0.01 * mask_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import show_points_and_masks_on_image\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = ''\n",
    "DATASET = 'coco'\n",
    "MODEL = 'FastSAM'\n",
    "ROOT = Path(\"../Datasets/coco-2017/val2017/\") if DATASET == 'coco' else Path(\"../Datasets/Cityscapes/leftImg8bit/val/\")\n",
    "SPARSITY = 50\n",
    "CLASSES = ['background', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant',\n",
    "           'street sign', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', \n",
    "           'hat', 'backpack', 'umbrella', 'shoe', 'eye glasses', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports', 'kite', \n",
    "           'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'plate', 'wine glass', 'cup', 'fork', 'knife', \n",
    "           'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "           'potted plant', 'bed', 'mirror', 'dining table', 'window', 'desk', 'toilet', 'door', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "           'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'blender', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "           'hair drier', 'toothbrush', 'hair brush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(target, pred, eps=1e-5, verbose=False):\n",
    "\n",
    "    if verbose:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(target)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(pred)\n",
    "        plt.show()\n",
    "\n",
    "    output = np.reshape(pred, -1)\n",
    "    target = np.reshape(target, -1)\n",
    "\n",
    "    tp = np.sum(output * target)  # TP (Intersection)\n",
    "    un = np.sum(output + target)  # Union\n",
    "    fp = np.sum(output * (~target))  # FP\n",
    "    fn = np.sum((~output) * target)  # FN\n",
    "    tn = np.sum((~output) * (~target))  # TN\n",
    "\n",
    "    iou = (tp + eps) / (un + eps)\n",
    "    pixel_acc = (tp + tn + eps) / (tp + tn + fp + fn + eps)\n",
    "    dice = (2 * tp + eps) / (2 * tp + fp + fn + eps)\n",
    "    precision = (tp + eps) / (tp + fp + eps)\n",
    "    recall = (tp + eps) / (tp + fn + eps)\n",
    "    specificity = (tn + eps) / (tn + fp + eps)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"IoU: {iou:.4f}, Pixel Acc: {pixel_acc:.4f}, Dice: {dice:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return iou, pixel_acc, dice, precision, specificity, recall\n",
    "\n",
    "def get_analytics(target_df, pred_df):\n",
    "    metrics = {k: [] for k in ['name', 'prompt', 'class', 't_class', 's_class', 'score', 'score_diff', 'mask_size', \n",
    "                               'mask_size_diff', 'iou', 'pixel_acc', 'dice', 'precision', 'recall', 'specificity']}\n",
    "    for i in range(len(target_df)):\n",
    "        target = target_df.loc[i]\n",
    "        pred = pred_df.loc[i]\n",
    "\n",
    "        iou, pixel_acc, dice, precision, specificity, recall = calculate_metrics(target['mask'], pred['mask'])\n",
    "        \n",
    "        metrics['name'].append(target['name'])\n",
    "        metrics['prompt'].append(target['prompt'])\n",
    "        metrics['class'].append(target['class'])\n",
    "        metrics['t_class'].append(target['s_class'])\n",
    "        metrics['s_class'].append(pred['s_class'])\n",
    "        metrics['score'].append(pred['score'])\n",
    "        metrics['score_diff'].append((pred['score'] - target['score']) / (target['score'] + 1e-5))\n",
    "        p_size = np.mean(pred['mask'].astype('float'))\n",
    "        t_size = np.mean(target['mask'].astype('float'))\n",
    "        metrics['mask_size'].append(p_size)\n",
    "        metrics['mask_size_diff'].append((p_size - t_size) / (t_size + 1e-3))\n",
    "        metrics['iou'].append(iou)\n",
    "        metrics['pixel_acc'].append(pixel_acc)\n",
    "        metrics['dice'].append(dice)\n",
    "        metrics['precision'].append(precision)\n",
    "        metrics['recall'].append(recall)\n",
    "        metrics['specificity'].append(specificity)\n",
    "    \n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "def get_labels(name):\n",
    "    if isinstance(name, list):\n",
    "        return [get_labels(n) for n in name]\n",
    "    else: \n",
    "        return CLASSES[name].title()\n",
    "\n",
    "def get_image(name):\n",
    "    if DATASET == 'coco':\n",
    "        image_path = ROOT.joinpath(f'{str(name).zfill(12)}.jpg')\n",
    "    else:\n",
    "        image_path = ROOT.joinpath(f\"{name.split('_')[0]}/{name}\")\n",
    "    return np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "\n",
    "def show_entry(row, target_df, pred_df):\n",
    "    image = get_image(row['name'])\n",
    "    target_mask = target_df[target_df['name']==row['name']]['mask'].values[0]\n",
    "    pred_mask = pred_df[pred_df['name']==row['name']]['mask'].values[0]\n",
    "    show_points_and_masks_on_image(image, [pred_mask, target_mask], [row['prompt']])\n",
    "    print(f'ID: {row[\"name\"]}, PromptClass: {get_labels(row[\"class\"])}, TargetClass: {get_labels(row[\"t_class\"])}, PredClass: {get_labels(row[\"s_class\"])},') \n",
    "    print(f'ScoreDiff: {row[\"score_diff\"]:.4f}, MaskSizeDiff: {row[\"mask_size_diff\"]:.4f}, IoU: {row[\"iou\"]:.4f}')\n",
    "    \n",
    "def show_samples(pie_df, target_df, pred_df, n=5):\n",
    "    print('Legend: Target -> Orange, Prediction -> Blue')\n",
    "    pie_df.iloc[:n].apply(lambda x: show_entry(x, target_df, pred_df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.read_pickle(f\"results/{EXPERIMENT}{DATASET}_prompts.pkl\")\n",
    "df_0 = pd.read_pickle(f\"results/{EXPERIMENT}{DATASET}_SAM_0.pkl\")\n",
    "df_s = pd.read_pickle(f\"results/{EXPERIMENT}{DATASET}_{MODEL}_0.pkl\")\n",
    "df_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0s = get_analytics(df_0, df_s)\n",
    "df_0s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_size = df_0s.nsmallest(25, ['mask_size_diff'])\n",
    "max_size = df_0s.nlargest(25, ['mask_size_diff'])\n",
    "min_score = df_0s.nsmallest(25, ['score_diff']) # not very useful\n",
    "max_score = df_0s.nlargest(25, ['score_diff']) # not very useful\n",
    "min_iou = df_0s.nsmallest(25, ['iou'])\n",
    "max_iou = df_0s.nlargest(25, ['iou'])\n",
    "min_size.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iou.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(min_size, df_0, df_s, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s[df_s['score']<=0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s.hist(column='score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.zeros((224,224), dtype=bool)\n",
    "o = torch.ones((50,50), dtype=bool)\n",
    "m[100:150, :50] = o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(m)\n",
    "x, y = torch.argwhere(m==1).sum(0)/torch.sum(m)\n",
    "x, y = int(x), int(y)\n",
    "print(x, y)\n",
    "plt.scatter(y, x, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
