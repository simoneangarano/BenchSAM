{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "from utils.utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "'EXPERIMENT': 'center_',\n",
    "'DATASET': 'sa1b',\n",
    "'MODEL': 'MobileSAM_iou_lr',\n",
    "'TARGET': 'SAM',\n",
    "'SPARSITY': 0,\n",
    "'CLASS': 0,\n",
    "'MODE': '',\n",
    "'METRIC': 'iou'\n",
    "}\n",
    "cfg['ROOT'], cfg['N'], cfg['CLASSES'], cfg['SUPERCLASSES'], cfg['SUP_N'] = get_dataset_info(cfg['DATASET'])\n",
    "cfg['SUP_L'] = {v: k for k, v in cfg['SUP_N'].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.read_pickle(f\"../results/{cfg['EXPERIMENT']}{cfg['DATASET']}_prompts.pkl\")\n",
    "df_0 = pd.read_pickle(f\"../results/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['TARGET']}_0.pkl\")\n",
    "df_s = pd.read_pickle(f\"../results/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['MODEL']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Image Shape\n",
    "if 'shape' not in df_p.keys():\n",
    "    df_p = add_image_shape(df_p, cfg)\n",
    "    df_p.head()\n",
    "    df_p.to_pickle(f\"../results/{cfg['EXPERIMENT']}{cfg['DATASET']}_prompts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p['score'] = df_p.apply(lambda s: 0, axis=1)\n",
    "df_p['s_class'] = df_p.apply(lambda s: None, axis=1)\n",
    "# df_p['mask_size'] = df_p.apply(get_mask_size, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_0['shape'] = df_0.apply(lambda s: s['mask']['size'], axis=1)\n",
    "#df_0['mask_size'] = df_0.apply(get_mask_size, axis=1)\n",
    "#df_0.to_pickle(f\"../results/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['TARGET']}_0.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 900\n",
    "plt.imshow(get_image(df_p['name'][i], cfg))\n",
    "plt.scatter(*df_p['prompt'][i], c='y', s=50)\n",
    "#plt.imshow(get_full_mask(df_s['mask'][i], df_s['mask_origin'][i], df_p['shape'][i]), alpha=0.6, cmap='Blues')\n",
    "plt.imshow(get_full_mask(df_0['mask'][i], df_0['mask_origin'][i], df_p['shape'][i]), alpha=0.6, cmap='Reds')\n",
    "#plt.imshow(get_full_mask(df_p['mask'][i], df_p['mask_origin'][i], df_p['shape'][i]), alpha=0.6, cmap='Greens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAM vs GT\n",
    "if os.path.exists(f\"../results/analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['TARGET']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\"):\n",
    "    df_g0 = pd.read_pickle(f\"../results/analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['TARGET']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\")\n",
    "else:\n",
    "    df_g0 = get_analytics(df_p, df_0, df_p, cfg, skip_empty=True)\n",
    "    df_g0.to_pickle(f\"../results/analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['TARGET']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\")\n",
    "    df_g0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileSAM vs SAM\n",
    "if os.path.exists(f\"../results/analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['MODEL']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\"):\n",
    "    df_0s = pd.read_pickle(f\"../results/analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['MODEL']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\")\n",
    "else:\n",
    "    df_0s = get_analytics(df_0, df_s, df_p, cfg, skip_empty=True)\n",
    "    df_0s.to_pickle(f\"../results/analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['MODEL']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\")\n",
    "    df_0s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileSAMft vs SAM\n",
    "df_0s.hist(column='iou')\n",
    "plt.semilogy()\n",
    "plt.show()\n",
    "df_0s['iou'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileSAMft vs SAM\n",
    "df_0s.hist(column='mask_size_diff')\n",
    "plt.semilogy()\n",
    "plt.show()\n",
    "df_0s['mask_size_diff'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Save Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_analytics(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_summary(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Summary\n",
    "summary = []\n",
    "for n in MODELS[1:]:\n",
    "    cfg['MODEL'] = n\n",
    "    summary.append(get_summary(cfg))\n",
    "summary = pd.concat(summary)\n",
    "summary.to_pickle(f\"analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_summary.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['MODE'] = ''\n",
    "cfg['SPARSITY'] = 0\n",
    "for t in METRICS:\n",
    "    cfg['METRIC'] = t\n",
    "    summary = []\n",
    "    for n in MODELS[1:]:\n",
    "        cfg['MODEL'] = n\n",
    "        summary.append(get_summary(cfg))\n",
    "    summary = pd.concat(summary)\n",
    "    get_hists(summary, cfg, save=True, plot=True)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.read_pickle(f\"analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_summary.pkl\")\n",
    "\n",
    "for m in METRICS:\n",
    "    cfg['METRIC'] = m\n",
    "    get_curves(summary, cfg, std=False, plot=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class-wise Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Summary\n",
    "summary = []\n",
    "for m in MODES:\n",
    "    cfg['MODE'] = m\n",
    "    for s in SPARSITIES:\n",
    "        cfg['SPARSITY'] = s\n",
    "        summary.append(get_summary(cfg, classwise=True))\n",
    "\n",
    "cfg['MODE'] = ''\n",
    "cfg['SPARSITY'] = 0\n",
    "for n in MODELS[1:]:\n",
    "    cfg['MODEL'] = n\n",
    "    summary.append(get_summary(cfg, classwise=True))\n",
    "summary = pd.concat(summary)\n",
    "summary.to_pickle(f\"analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_classwise.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = summary[summary['pruning']=='_gup']\n",
    "d = d[d['id']==50]\n",
    "d = d[d['class']!=0]\n",
    "get_hists(d, cfg, save=False, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class-wise Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.read_pickle(f\"analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_classwise.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classwise_curves(d, cfg, plot=True, save=False):\n",
    "    _, ax = plt.subplots()\n",
    "    ax.grid()\n",
    "    ax.title.set_text(cfg['METRIC'].replace('_', ' ').capitalize())\n",
    "    ax.plot(PARAMS['SAM'], SAM[cfg['METRIC']], 'o')\n",
    "     \n",
    "    for c in range(1, d.shape[1]):\n",
    "        if cfg['MODEL'] == 'SAM':\n",
    "            p = (1 - d[:,c,1]/100) * PARAMS['SAM'] \n",
    "        else:\n",
    "            p = PARAMS[d[:,c,1][0]]\n",
    "        ax.plot(p, d[:,c,2], '-o', label=f\"{cfg['SUP_L'][c]} ({d[0,c,3]})\")\n",
    "    # Shrink current axis's height by 10% on the bottom\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), fancybox=False, shadow=False, ncol=4, fontsize=9)   \n",
    "    #plt.legend(loc='best', fontsize=9, ncol=2)\n",
    "    plt.xlabel('Parameters')\n",
    "    plt.ylabel(cfg['METRIC'].replace('_', ' ').capitalize())\n",
    "    plt.axvline(PARAMS['SAM'], linestyle='--')\n",
    "    plt.axhline(SAM[cfg['METRIC']], linestyle='--')\n",
    "    plt.semilogx()\n",
    "    if save:\n",
    "        plt.savefig(f\"figures/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['METRIC']}_{cfg['MODEL']}{cfg['MODE']}_curves_classwise.pdf\", bbox_inches='tight')\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUP\n",
    "for m in METRICS:\n",
    "    cfg['METRIC'] = m\n",
    "    d = summary[summary['pruning']=='_gup'][['class', 'id', cfg['METRIC'], 'n']].values.reshape(9, 13, -1)\n",
    "    get_classwise_curves(d, cfg, plot=True, save=True)\n",
    "\n",
    "# SparseGPT\n",
    "cfg['MODE'] = '_gup'\n",
    "for m in METRICS:\n",
    "    cfg['METRIC'] = m\n",
    "    d = summary[summary['id'].str.isalpha().isnull()]\n",
    "    d = d[d['pruning']!='_gup'][['class', 'id', cfg['METRIC'], 'n']].values.reshape(9, 13, -1)\n",
    "    get_classwise_curves(d, cfg, plot=True, save=True)\n",
    "\n",
    "# FastSAM\n",
    "# cfg['MODEL'] = 'FastSAM'\n",
    "# for m in METRICS:\n",
    "#     cfg['METRIC'] = m\n",
    "#     d = summary[summary['id']==cfg['MODEL']][['class', 'id', cfg['METRIC'], 'n']].values.reshape(1, 13, -1)\n",
    "#     get_classwise_curves(d, cfg, plot=True, save=True)\n",
    "\n",
    "# MobileSAM\n",
    "# cfg['MODEL'] = 'MobileSAM'\n",
    "# for m in METRICS:\n",
    "#     cfg['METRIC'] = m\n",
    "#     d = summary[summary['id']==cfg['MODEL']][['class', 'id', cfg['METRIC'], 'n']].values.reshape(1, 13, -1)\n",
    "#     get_classwise_curves(d, cfg, plot=True, save=True)\n",
    "\n",
    "# Mobile&FastSAM\n",
    "cfg['MODEL'] = 'Mobile&FastSAM'\n",
    "for m in METRICS:\n",
    "    cfg['METRIC'] = m\n",
    "    d = summary[summary['id'].str.isalpha().notnull()][['class', 'id', cfg['METRIC'], 'n']].values.reshape(1, 13, -1)\n",
    "    get_classwise_curves(d, cfg, plot=True, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forgettability_curve_dissimilarity(d, cfg, plot=True, save=False):\n",
    "    fcd = np.zeros((d.shape[1], d.shape[1]))\n",
    "    for c1 in range(d.shape[1]):\n",
    "        for c2 in range(c1, d.shape[1]):\n",
    "            if c1 == c2:\n",
    "                continue\n",
    "            fcd[c1, c2] = ((d[:,c1,2] - d[:,c2,2])**2).mean()\n",
    "            fcd[c2, c1] = fcd[c1, c2]\n",
    "\n",
    "    plt.imshow(fcd, interpolation='nearest')\n",
    "    plt.xticks(range(12), [cfg['SUP_L'][i] for i in range(1, 13)], rotation=90)\n",
    "    plt.yticks(range(12), [cfg['SUP_L'][i] for i in range(1, 13)])\n",
    "    plt.title('Forgettability Curve Dissimilarity - GUP')\n",
    "    plt.colorbar()\n",
    "    if save:\n",
    "        plt.savefig(f\"figures/{cfg['EXPERIMENT']}{cfg['DATASET']}{cfg['MODE']}_fcd.pdf\", bbox_inches='tight')\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg['MODE'] == '':\n",
    "    # SparseGPT\n",
    "    d = summary[summary['id'].str.isalpha().isnull()]\n",
    "    d = d[d['pruning']!='_gup'][['class', 'id', cfg['METRIC'], 'n']].values.reshape(9, 13, -1)[:,1:,:]\n",
    "elif cfg['MODE'] == '_gup':\n",
    "    # GUP\n",
    "    d = summary[summary['pruning']=='_gup'][['class', 'id', cfg['METRIC']]].values.reshape(9, 13, -1)[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_forgettability_curve_dissimilarity(d, cfg, plot=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(f\"analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['MODEL']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column='iou')\n",
    "plt.semilogy()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_superclass(df, cfg)\n",
    "df['mask_size_target'] = (df['mask_size'] - df['mask_size_diff'] * 1e-5) / (1 + df['mask_size_diff'])\n",
    "df['f1'] = 2 * df['precision'] * df['recall'] / (df['precision'] + df['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['iou', 'precision', 'recall', 'mask_size', 'mask_size_diff', 'superclass']\n",
    "data = df[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'] = get_clusters(data, cfg, plot=True, save=False)\n",
    "#df.to_pickle(f\"analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['MODEL']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def prod(l1, l2):\n",
    "   return list(product(l1, l2))\n",
    "\n",
    "M = ['iou', 'precision', 'recall']\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, sharex=True, sharey='row', figsize=(10,8))\n",
    "for (ax, (c,m)) in zip(axs.flat, prod(range(3), M)):\n",
    "    y, x = np.histogram(df[df['cluster']==c][m])\n",
    "    ax.bar(x[:-1], y, width=x[1]-x[0], alpha=1)\n",
    "    ax.set_title(f\"[{c}] {m}\", fontsize=10)\n",
    "    ax.semilogy()\n",
    "plt.suptitle(f\"{cfg['MODEL']} {cfg['MODE']}\", fontsize=12)\n",
    "plt.savefig(f\"figures/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['MODEL']}_{cfg['SPARSITY']}{cfg['MODE']}_clusters.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = pd.read_pickle(f\"../results/analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['MODEL']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\")\n",
    "df_0 = pd.read_pickle(f\"../results/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['TARGET']}_0.pkl\")\n",
    "df_s = pd.read_pickle(f\"../results/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['MODEL']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = int(0.05*len(df_c))\n",
    "min_iou = df_c.nsmallest(N,\n",
    "                         ['iou'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c['iou'].mean(), min_iou['iou'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(pie_df=min_iou, target_df=df_0, pred_df=df_s, cfg=cfg, prompt_zoom=False, n=5, random=True, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(f\"../results/analytics/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['MODEL']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\")\n",
    "df_0 = pd.read_pickle(f\"../results/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['TARGET']}_0.pkl\")\n",
    "df_s = pd.read_pickle(f\"../results/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['MODEL']}_{cfg['SPARSITY']}{cfg['MODE']}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out SAM's wrong masks (out of prompt)\n",
    "df_0['check'] = df_0.apply(lambda s: check_prompt(s), axis=1)\n",
    "df_0.hist(column='check', range=(0,.1))\n",
    "\n",
    "df_0 = df_0[df_0['check']!=0]\n",
    "df = df[(df.name.isin(df_0.name))]\n",
    "df_s = df_s[(df_s.name.isin(df_0.name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s['check'] = df_s.apply(lambda s: check_prompt(s), axis=1)\n",
    "df_s.hist(column='check', range=(0,.1))\n",
    "\n",
    "df_s = df_s[df_s['check']!=0]\n",
    "df = df[(df.name.isin(df_s.name))]\n",
    "df_0 = df_0[(df_0.name.isin(df_s.name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mask_size_target'] = (df['mask_size'] - df['mask_size_diff'] * 1e-5) / (1 + df['mask_size_diff'])\n",
    "df['f1'] = 2 * df['precision'] * df['recall'] / (df['precision'] + df['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = int(0.1*len(df))\n",
    "min_iou = df.nsmallest(N, ['iou']) # [df['iou']>0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.ones_like(df['mask_size_target']) / len(df['mask_size_target'])\n",
    "df.hist(column='mask_size_target', range=(0,1), weights=weights, cumulative=True, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.ones_like(min_iou['mask_size_target']) / len(min_iou['mask_size_target'])\n",
    "min_iou.hist(column='mask_size_target', range=(0,1), weights=weights, cumulative=True, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(len(df[df['mask_size_diff'] < -0.5]) / len(df) * 100)\n",
    "print(len(df[df['mask_size_diff'] > 0.5]) / len(df) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(min_iou))\n",
    "print(len(min_iou[min_iou['mask_size_diff'] < -0.5]) / len(min_iou) * 100)\n",
    "print(len(min_iou[min_iou['mask_size_diff'] > 0.5]) / len(min_iou) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column='mask_size', bins=100)\n",
    "plt.semilogy()\n",
    "df['mask_size'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_iou.hist(column='mask_size', bins=100)\n",
    "plt.semilogy()\n",
    "#plt.ylim(1, 350)\n",
    "plt.show()\n",
    "min_iou['mask_size'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_iou.sort_values(by=['f1'], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(pie_df=min_iou, target_df=df_0, pred_df=df_s, cfg=cfg, prompt_zoom=False, thr=50, n=10, random=True, save=False, zoom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pycocotools import mask as mask_utils\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.read_pickle(f\"../results/{cfg['EXPERIMENT']}{cfg['DATASET']}_prompts.pkl\")\n",
    "df_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anns(name, cfg):\n",
    "    ann_path = cfg['ROOT'].joinpath(f\"sa_000020/{name}.json\")\n",
    "    with open(ann_path, 'r') as ann_path:\n",
    "        return json.load(ann_path)\n",
    "    \n",
    "def prompt_in_bbox(prompt, bbox):\n",
    "    return prompt[0] > bbox[0] and prompt[1] > bbox[1] and prompt[0] < bbox[0]+bbox[2] and prompt[1] < bbox[1]+bbox[3]\n",
    "\n",
    "def prompt_in_mask(prompt, ann):\n",
    "    if prompt_in_bbox(prompt, ann['bbox']):\n",
    "        mask = mask_utils.decode(ann['segmentation'])\n",
    "        if mask[prompt[1], prompt[0]]:\n",
    "            return mask\n",
    "    return\n",
    "\n",
    "def get_ground_truth_mask(s, cfg):\n",
    "    prompt = s['prompt']\n",
    "    anns = get_anns(s['name'], cfg)['annotations']\n",
    "    for ann in anns:\n",
    "        mask = prompt_in_mask(prompt, ann)\n",
    "        if mask is not None:\n",
    "            return mask.astype(bool)\n",
    "    print(s['name'], ' No mask found')\n",
    "    return np.zeros((s['shape'][0], s['shape'][1])).astype(bool)\n",
    "\n",
    "def get_ground_truth(df, cfg):\n",
    "    df['gt'] = df.apply(lambda s: get_mask_limits([get_ground_truth_mask(s, cfg)]), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = get_ground_truth(df_p, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.iloc[0]['gt'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p['mask'] = df_p.apply(lambda s: s['gt'][2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p['mask_origin'] = df_p.apply(lambda s: s['gt'][0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p['gt_origin'] = df_p.apply(lambda s: s['gt_origin'][::-1], axis=1)\n",
    "#df_p['gt'] = df_p.apply(lambda s: s['gt'][2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(df_p['gt'][900])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(get_full_mask(df_p['gt'][900], df_p['gt_origin'][900], df_p['shape'][900]), alpha=0.6, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.to_pickle(f\"../results/{cfg['EXPERIMENT']}{cfg['DATASET']}_prompts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wrong_prompt(s, cfg):\n",
    "    if not s['mask'].any():\n",
    "        print(s['name'])\n",
    "        #show_points_on_image(get_image(s['name'], cfg), [s['prompt']], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.apply(lambda s: plot_wrong_prompt(s, cfg), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p[df_p['name']=='sa_229068']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_g0[df_g0['name']=='sa_229068']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_g0.sort_values(by=['mask_size'], ascending=True, inplace=True)\n",
    "#df_g0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../Datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(label):\n",
    "    show_points_on_image(label.bool(), [[0,0]], input_labels=None)\n",
    "    e = cv2.Canny(image=label.numpy().astype(np.uint8), threshold1=10, threshold2=50)\n",
    "    e = cv2.dilate(e, np.ones((10, 10), np.uint8), iterations = 1).astype(bool)\n",
    "    label[e] = 0\n",
    "    C = np.unique(label)[1:]\n",
    "    if len(C) == 0:\n",
    "        c = 0\n",
    "    else:\n",
    "        c = np.random.choice(C)\n",
    "    x_v, y_v = np.where(label == c)\n",
    "    r = random.randint(0, len(x_v) - 1)\n",
    "    x, y = x_v[r], y_v[r]\n",
    "    show_points_on_image(label.bool(), [[y,x]], input_labels=None)\n",
    "\n",
    "    return [[[y,x]]], c # inverted to compensate different indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = COCOSegmentation(root=data_dir.joinpath('coco-2017/'), split='val', crop_size=0)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, \n",
    "                                         num_workers=4, pin_memory=True, worker_init_fn=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, (i, l, n) in enumerate(dataloader):\n",
    "    prompt, p_class = get_prompt(l[0])\n",
    "    print(prompt)\n",
    "    #if l[0][prompt[0][0][1], prompt[0][0][0]] == 0 and l[0].any():\n",
    "    print(n)\n",
    "    plt.imshow(i[0])\n",
    "    plt.imshow(l[0].bool(), alpha=.5)\n",
    "    show_points_and_masks_on_image(i[0], l.bool(), prompt[0], input_labels=None, zoom=True, prompt_zoom=True, thr=20, save=None)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image = Image.open(data_dir.joinpath('coco-2017/val2017/000000000139.jpg')).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(device)\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_points = [[[514, 216]]]\n",
    "show_points_on_image(raw_image, input_points[0])\n",
    "\n",
    "inputs = processor(raw_image, input_points=input_points, return_tensors=\"pt\").to(device)\n",
    "image_embeddings = model.get_image_embeddings(inputs[\"pixel_values\"])\n",
    "\n",
    "\n",
    "inputs.pop(\"pixel_values\", None) # pixel_values are no more needed\n",
    "inputs.update({\"image_embeddings\": image_embeddings})\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = processor.image_processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu())[0]\n",
    "scores = outputs.iou_scores\n",
    "show_masks_on_image(raw_image, masks, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sam = masks[0,scores.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(raw_image)\n",
    "plt.imshow(mask_sam, alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"vit_t\"\n",
    "sam_checkpoint = \"bin/mobile_sam.pt\"\n",
    "\n",
    "model = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(device).eval()\n",
    "predictor = SamPredictor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_label = np.array([1])\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictor.set_image(np.array(raw_image))\n",
    "    masks, scores, _ = predictor.predict(np.array(input_points[0]), input_label)\n",
    "\n",
    "plt.imshow(raw_image)\n",
    "plt.imshow(mask_sam, alpha=0.5)\n",
    "plt.imshow(masks[np.argmax(scores)], alpha=0.5)\n",
    "plt.scatter(input_points[0][0][0], input_points[0][0][1], c='r', s=10)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.read_pickle(f\"results/{cfg['EXPERIMENT']}{cfg['DATASET']}_{cfg['TARGET']}_0.pkl\")\n",
    "df_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0['check'] = df_0.apply(lambda s: check_prompt(s), axis=1)\n",
    "df_invalid = df_0[df_0['check']==0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0.hist(column='check', range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invalid = df_0[df_0['check']==0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(pie_df=df_invalid, target_df=df_0, pred_df=df_s, cfg=cfg, prompt_zoom=True, n=5, random=True, save=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
