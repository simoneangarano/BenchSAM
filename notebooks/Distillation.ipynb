{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import (SamModel, SamProcessor)\n",
    "from utils.mobile_sam import sam_model_registry\n",
    "from utils.mobile_sam.predictor import SamPredictor\n",
    "\n",
    "from utils.datasets import SA1B_Dataset\n",
    "from utils.utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "DATA_DIR = Path('../Datasets/')\n",
    "GPU = 3\n",
    "\n",
    "DEVICE = torch.device(f\"cuda:{GPU}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = SA1B_Dataset(root=DATA_DIR.joinpath('SA_1B/images/'), features=None, split='sa_000009')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, (i, l, n) in enumerate(dataloader):\n",
    "    print(i.shape, l.shape, n)\n",
    "    if j > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(l.unique()))\n",
    "plt.imshow(l[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher\n",
    "teacher = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(DEVICE).eval()\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student\n",
    "model_type = \"vit_t\"\n",
    "sam_checkpoint = \"bin/mobile_sam.pt\"\n",
    "\n",
    "model = sam_model_registry[model_type](checkpoint=None).to(DEVICE).train()\n",
    "student = SamPredictor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = processor(i, input_points=None, return_tensors=\"pt\").to(DEVICE)\n",
    "    t_features = teacher.get_image_embeddings(inputs[\"pixel_values\"]).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse\n",
    "mse = torch.nn.MSELoss()\n",
    "mse(t_features, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller():\n",
    "    def __init__(self, teacher, student, processor, dataloader, optimizer, device):\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "        self.processor = processor\n",
    "        self.dataloader = dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "\n",
    "    def get_distillation_loss(self, img):\n",
    "        student.set_image(img[0].permute((2,0,1)))\n",
    "        s_features = student.features\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = processor(img, input_points=None, return_tensors=\"pt\").to(DEVICE)\n",
    "            t_features = teacher.get_image_embeddings(inputs[\"pixel_values\"])\n",
    "\n",
    "        return torch.nn.functional.mse_loss(s_features, t_features)\n",
    "\n",
    "    def distill(self):\n",
    "        t = tqdm(dataloader, desc='Distillation:')\n",
    "        for img, _, _ in t:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.get_distillation_loss(img)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            t.set_postfix({'Loss': loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(student.model.parameters(), lr=1e-3)\n",
    "distiller = Distiller(teacher, student, processor, dataloader, optimizer, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiller.distill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(distiller.student.model.state_dict(), 'bin/distilled_mobile_sam.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_features_ids = csv.reader(open(Path('results/feature_ids.csv'), 'r'))\n",
    "l = list(teacher_features_ids)\n",
    "#l = [i[0] for i in l]\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.load('results/teacher_features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prompt a random point belonging to an instance\n",
    "- get the corresponding mask and mask size\n",
    "- use saved SAM features\n",
    "- freeze MobileSAM backbone\n",
    "- prompt SAM and MobileSAM and collect output masks (3 masks?)\n",
    "- compute dice and focal loss (20:1)\n",
    "- weight loss based on mask size\n",
    "- OBTAIN LOGITS FROM (MOBILESAM, SAM) !!! (return_logits=True, binarize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_prompt()\n",
    "# get_instance_label()\n",
    "# get_mask_size()\n",
    "# size coefficient = 1 - (mask_size / image_size)\n",
    "#Â get_output(SAM, saved_features, prompt)\n",
    "# get_output(MobileSAM, saved_features, prompt) \n",
    "# dice_loss()\n",
    "# focal_loss()\n",
    "# loss = (20 * dice_loss() + focal_loss()) * size_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distillation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../Datasets/')\n",
    "SPLIT = 'sa_000020'\n",
    "GPU = 2\n",
    "DEVICE = torch.device(f\"cuda:{GPU}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "SHUFFLE = True\n",
    "LOAD_FEATURES = True\n",
    "FEATURES = 'results/teacher_features.pt' if LOAD_FEATURES else None\n",
    "\n",
    "EPOCHS = 16\n",
    "LR = 1e-3\n",
    "OPTIM = 'adamw'\n",
    "WD = 1e-5\n",
    "LOSS_WEIGHTS = [0,0,1,0] # 20 focal, 1 dice, 0 bce, 0 size\n",
    "\n",
    "MODE = 'decoder' # encoder, decoder, save_features\n",
    "PRETRAINED = True if MODE == 'decoder' else False\n",
    "\n",
    "dataset = SA1B_Dataset(root=DATA_DIR.joinpath('SA_1B/images/'), split=SPLIT,  features=FEATURES, labels=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=SHUFFLE, num_workers=16, pin_memory=True)\n",
    "\n",
    "teacher = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(DEVICE)\n",
    "teacher.eval()\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "\n",
    "model_type = \"vit_t\"\n",
    "sam_checkpoint = \"bin/mobile_sam.pt\" if PRETRAINED else None\n",
    "\n",
    "model = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(DEVICE)\n",
    "model.eval()\n",
    "for m in model.image_encoder.modules():\n",
    "    if isinstance(m, torch.nn.BatchNorm2d):\n",
    "        m.eval()\n",
    "        m.weight.requires_grad_(False)\n",
    "        m.bias.requires_grad_(False)\n",
    "student = SamPredictor(model)\n",
    "\n",
    "if MODE == 'encoder':\n",
    "    DISTILLER = EncDistiller\n",
    "    params = student.model.image_encoder.parameters()\n",
    "else:\n",
    "    DISTILLER = DecDistiller\n",
    "    params = student.model.mask_decoder.parameters()\n",
    "\n",
    "if OPTIM == 'adamw':\n",
    "    optimizer = torch.optim.AdamW(params, lr=LR, weight_decay=WD)\n",
    "elif OPTIM == 'adam':\n",
    "    optimizer = torch.optim.Adam(params, lr=LR)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "distiller = DISTILLER(teacher, student, processor, dataloader, optimizer, scheduler, loss_weights=LOSS_WEIGHTS, device=DEVICE)\n",
    "\n",
    "if MODE == 'save_features':\n",
    "    distiller.save_teacher_features(Path('results/teacher_features.pt'))\n",
    "else:\n",
    "    distiller.distill(epochs=EPOCHS, accumulate=BATCH_SIZE, use_saved_features=LOAD_FEATURES, name=MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pstats import Stats\n",
    "\n",
    "filename = '../results/profile.prof' \n",
    "my_stat = Stats(filename, stream=sys.stdout).sort_stats('time')\n",
    "my_stat.print_stats(.1)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distiller Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import (SamModel, SamProcessor)\n",
    "from utils.mobile_sam import sam_model_registry\n",
    "from utils.mobile_sam.predictor import SamPredictor\n",
    "from utils.datasets import SA1B_Dataset\n",
    "from utils.utils import *\n",
    "from utils.distill_utils import *\n",
    "\n",
    "from ipywidgets import Button\n",
    "import asyncio\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_change(widget):\n",
    "    future = asyncio.Future()\n",
    "    def getvalue(change):\n",
    "        future.set_result(change.description)\n",
    "        widget.on_click(getvalue, remove=True) \n",
    "        # we need to free up the binding to getvalue to avoid an InvalidState error\n",
    "        # buttons don't support unobserve\n",
    "        # so use `remove=True` \n",
    "    widget.on_click(getvalue)\n",
    "    return future\n",
    "\n",
    "class InspectDistiller(DecDistiller):\n",
    "    def __init__(self, teacher, student, processor, dataloader, test_dataloader, optimizer, scheduler, loss_weights=[0,0,0,0,1,0], \n",
    "                 n_prompts=1, random_prompt=True, edge_filter=True, size_thr=0, profile=False, device='cuda', button=None):\n",
    "        super().__init__(teacher, student, processor, dataloader, test_dataloader, optimizer, scheduler, loss_weights, \n",
    "                         n_prompts, random_prompt, edge_filter, size_thr, profile, device)\n",
    "        self.BUTTON = button\n",
    "        \n",
    "    '''\n",
    "    def get_loss(self, t_mask, s_mask, label):\n",
    "        t_mask_bin = (t_mask > 0.0)\n",
    "        focal = self.focal_loss(s_mask, t_mask_bin.float())\n",
    "        bce = self.bce_loss(s_mask, t_mask_bin.float())\n",
    "        iou = self.iou_loss(s_mask, t_mask_bin.int())\n",
    "        dice = self.dice_loss(s_mask, t_mask_bin.int())\n",
    "        #bce_gt = self.bce_loss(s_mask, label.float())\n",
    "        #iou_gt = self.iou_loss(s_mask, label.int())\n",
    "        #bound = self.bound_loss(s_mask, label.int())\n",
    "        return focal, bce, iou, dice\n",
    "\n",
    "    def get_prompt(self, label, seed=None):\n",
    "        C, counts = np.unique(label.cpu(), return_counts=True)\n",
    "        counts = counts / counts.sum()\n",
    "        C = C[counts > 0.001]\n",
    "        # print(C)\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        c = np.random.choice(C)\n",
    "\n",
    "        x_v, y_v = np.where(label.cpu() == c)\n",
    "        if self.random_prompt:\n",
    "            if seed is not None:\n",
    "                random.seed(seed)\n",
    "            r = random.randint(0, len(x_v) - 1)\n",
    "            x, y = x_v[r], y_v[r]\n",
    "        else: # central prompt\n",
    "            x, y = x_v.mean(), y_v.mean()\n",
    "            x, y = int(x), int(y)\n",
    "        return [[[y,x]]], c # inverted to compensate different indexing\n",
    "    '''\n",
    "\n",
    "    def training_step(self, i, img, label, feats, acc, use_saved_features=False):\n",
    "        feats = feats.to(self.device) if use_saved_features else None\n",
    "        self.get_features(img)\n",
    "        prompts = self.get_prompts(label[0], seed=8)\n",
    "        label = label.to(self.device)\n",
    "        # print(prompts)\n",
    "\n",
    "        for prompt, c in prompts:\n",
    "            # show_points_on_image(img[0], prompt[0], np.array([1]), title='Prompt') \n",
    "            # show_points_on_image((label[0]==c).cpu().numpy(), prompt[0], np.array([1]), \n",
    "            #                      title=f'GT size:{(label[0]==c).cpu().numpy().sum()}')\n",
    "            t_mask, s_mask = self.get_masks(img, prompt, feats)\n",
    "            focal, bce, iou, dice = self.get_loss(t_mask, s_mask, label[0]==c)\n",
    "            loss = (self.FW * focal + self.BW * bce + self.IW * iou) / acc\n",
    "            loss.backward(retain_graph=True)\n",
    "            # print(f\"GT  BCE: {bce_gt.item():.2e} IoU: {iou_gt.item():.2e}\")\n",
    "            show_points_on_image(t_mask.cpu().numpy(), prompt[0], np.array([1]), title='Teacher Raw')\n",
    "            show_points_on_image(s_mask.detach().cpu().numpy(), prompt[0], np.array([1]), title='Student Raw') \n",
    "            show_points_on_image((t_mask>0).cpu().numpy(), prompt[0], np.array([1]), title='Teacher Mask') \n",
    "            show_points_on_image((s_mask>0).detach().cpu().numpy(), prompt[0], np.array([1]), title='Student Mask') \n",
    "            print(f\"BCE: {bce.item():.2e} IoU: {iou.item():.2e} Focal: {focal.item():.2e} Dice: {dice.item():.2e}\")\n",
    "            self.update_metrics(loss, focal, bce, iou, dice)\n",
    "\n",
    "        if (i+1) % acc == 0 or i+1 == len(self.dataloader):\n",
    "            print(f\"BCE: {self.r_bce/(i+1):.2e} IoU: {self.r_iou/(i+1):.2e} Focal: {self.r_focal/(i+1):.2e} Dice: {self.r_dice/(i+1):.2e}\")\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "    async def distill(self, epochs=8, acc=4, use_saved_features=False, name=''):\n",
    "        self.acc = acc\n",
    "        self.student.model.mask_decoder.train()\n",
    "        self.student.model.prompt_encoder.train()\n",
    "        self.student.model.image_encoder.eval()\n",
    "\n",
    "        self.init_metrics()\n",
    "        for i, (img, label, _, feats) in enumerate(self.dataloader):\n",
    "            self.training_step(i, img, label, feats, acc, use_saved_features)\n",
    "            print('Waiting for button click...')\n",
    "            _ = await wait_for_change(self.BUTTON)\n",
    "            clear_output(wait=True)\n",
    "            print('Button clicked!')\n",
    "            break\n",
    "\n",
    "        self.validate(use_saved_features=use_saved_features)\n",
    "\n",
    "    def validate(self, use_saved_features=False):\n",
    "        self.student.model.mask_decoder.eval()\n",
    "        self.student.model.prompt_encoder.eval()\n",
    "        self.student.model.image_encoder.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            r_iou = 0\n",
    "            for img, label, _, feats in self.test_dataloader:\n",
    "                feats = feats.to(self.device) if use_saved_features else None\n",
    "                self.get_features(img)\n",
    "                prompt, c = self.get_prompt(label[0], seed=0)\n",
    "                label = label.to(self.device)\n",
    "\n",
    "                t_mask, s_mask = self.get_masks(img, prompt, feats)\n",
    "                \n",
    "                show_points_on_image(t_mask.cpu().numpy(), prompt[0], np.array([1]), title='Teacher Raw')\n",
    "                show_points_on_image(s_mask.detach().cpu().numpy(), prompt[0], np.array([1]), title='Student Raw') \n",
    "                show_points_on_image((t_mask>0).cpu().numpy(), prompt[0], np.array([1]), title='Teacher Mask') \n",
    "                show_points_on_image((s_mask>0).detach().cpu().numpy(), prompt[0], np.array([1]), title='Student Mask') \n",
    "\n",
    "                focal, bce, iou, dice = self.get_loss(t_mask, s_mask, label[0]==c)\n",
    "                loss = self.FW * focal + self.BW * bce + self.IW * iou + self.DW * dice\n",
    "\n",
    "                iou = self.get_metrics(t_mask, s_mask, label[0]==c)\n",
    "                r_iou += iou\n",
    "                print(f\"Loss: {loss.item():.2e} BCE: {bce.item():.2e} IoU: {iou:.2e} Focal: {focal.item():.2e} Dice: {dice.item():.2e}\")\n",
    "                #print('Waiting for button click...')\n",
    "                #_ = await wait_for_change(self.BUTTON)\n",
    "                #clear_output(wait=True)\n",
    "                #print('Button clicked!')\n",
    "\n",
    "            return r_iou / len(self.test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config.yaml', 'r') as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "cfg['DATA_DIR'] = Path('../' + cfg['DATA_DIR'])\n",
    "cfg['DEVICE'] = torch.device(f\"cuda:{cfg['GPU']}\" if torch.cuda.is_available() else \"cpu\")\n",
    "cfg['PRETRAINED'] = True if cfg['MODE'] == 'decoder' else False\n",
    "DISTILLER = InspectDistiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SA1B_Dataset(root=cfg['DATA_DIR'].joinpath('SA_1B/images/'), split=[\"sa_00000\" + str(i) for i in range(cfg['TRAIN_SPLITS'])],\n",
    "                        features=None, labels=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=cfg['SHUFFLE'], num_workers=cfg['NUM_WORKERS'], pin_memory=False)\n",
    "test_dataset = SA1B_Dataset(root=cfg['DATA_DIR'].joinpath('SA_1B/images/'), split=[cfg['SPLIT']], \n",
    "                            features=None, labels=True, max=cfg['MAX_TEST'])\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=cfg['NUM_WORKERS'], pin_memory=False)\n",
    "\n",
    "teacher = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(cfg['DEVICE'])\n",
    "teacher.eval()\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "\n",
    "sam_checkpoint = \"../bin/mobile_sam.pt\" if cfg['PRETRAINED'] else None\n",
    "model = sam_model_registry[\"vit_t\"](checkpoint=sam_checkpoint, size_embedding=cfg['SIZE_EMBEDDING']).to(cfg['DEVICE'])\n",
    "model.eval()\n",
    "for m in model.image_encoder.modules():\n",
    "    if isinstance(m, torch.nn.BatchNorm2d):\n",
    "        m.eval()\n",
    "        m.weight.requires_grad_(False)\n",
    "        m.bias.requires_grad_(False)\n",
    "student = SamPredictor(model)\n",
    "\n",
    "params = list(student.model.mask_decoder.parameters()) + list(student.model.prompt_encoder.parameters())\n",
    "\n",
    "if cfg['OPTIM'] == 'adamw':\n",
    "    optimizer = torch.optim.AdamW(params, lr=cfg['LR'], weight_decay=cfg['WD'])\n",
    "if cfg['SCHEDULER'] == 'cos':\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg['EPOCHS'], eta_min=1.0e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiller = DISTILLER(teacher, student, processor, dataloader, test_dataloader, optimizer, scheduler, loss_weights=cfg['LOSS_WEIGHTS'], \n",
    "                      n_prompts=cfg['N_PROMPTS'], random_prompt=cfg['RANDOM_PROMPT'], edge_filter=cfg['EDGE_FILTER'], size_thr=cfg['SIZE_THR'],\n",
    "                      profile=cfg['PROFILE'], device=cfg['DEVICE'], button=Button(description='step()'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.create_task(distiller.distill(epochs=cfg['EPOCHS'], acc=cfg['BATCH_SIZE'],\n",
    "use_saved_features=cfg['LOAD_FEATURES'], name=f\"{cfg['MODE']}_{cfg['EXP']}\"))\n",
    "#asyncio.create_task(distiller.validate(use_saved_features=cfg['LOAD_FEATURES']))\n",
    "distiller.BUTTON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiller.BUTTON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAM in MobileSAM Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 7\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils.mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "from transformers import pipeline, SamModel, SamProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.4])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.4])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    del mask\n",
    "\n",
    "def show_masks_on_image(raw_image, masks):\n",
    "  plt.imshow(np.array(raw_image))\n",
    "  ax = plt.gca()\n",
    "  ax.set_autoscale_on(False)\n",
    "  for mask in masks:\n",
    "      show_mask(mask, ax=ax, random_color=False)\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()\n",
    "  del mask\n",
    "\n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original SAM\n",
    "SAM = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(DEVICE)\n",
    "SAM.eval()\n",
    "SAM_processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "SAM_gen = pipeline(\"mask-generation\", model=\"facebook/sam-vit-huge\", device=0)\n",
    "\n",
    "# MobileSAM's SAM\n",
    "model_type = \"vit_h\"\n",
    "sam_checkpoint = \"../bin/sam_vit_h_4b8939.pth\"\n",
    "TEST = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(DEVICE)\n",
    "TEST.eval()\n",
    "TEST_processor = SamPredictor(TEST)\n",
    "TEST_gen = SamAutomaticMaskGenerator(TEST)\n",
    "\n",
    "# MobileSAM's MobileSAM\n",
    "model_type = \"vit_t\"\n",
    "sam_checkpoint = \"../bin/mobile_sam.pt\"\n",
    "MOBILESAM = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(DEVICE)\n",
    "MOBILESAM.eval()\n",
    "MOBILESAM_processor = SamPredictor(MOBILESAM)\n",
    "MOBILESAM_gen = SamAutomaticMaskGenerator(MOBILESAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg\"\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = SAM_gen(raw_image, points_per_batch=64)\n",
    "masks = outputs[\"masks\"]\n",
    "show_masks_on_image(raw_image, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_masks = TEST_gen.generate(np.array(raw_image))\n",
    "sam_masks = [m['segmentation'] for m in sam_masks]\n",
    "show_masks_on_image(raw_image, sam_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilesam_masks = MOBILESAM_gen.generate(np.array(raw_image))\n",
    "mobilesam_masks = [m['segmentation'] for m in mobilesam_masks]\n",
    "show_masks_on_image(raw_image, mobilesam_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = []\n",
    "for s, m in zip(sam_masks, mobilesam_masks):\n",
    "    s = s.astype(float)\n",
    "    m = m.astype(float)\n",
    "    e = np.mean((s-m)**2)\n",
    "    mae.append(e)\n",
    "    if e > 1e-4:\n",
    "        plt.imshow(s, alpha=0.6)\n",
    "        plt.imshow(m, alpha=0.6)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "np.mean(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size Prompt Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "EMBED_DIM = 256\n",
    "CLASSES = 10\n",
    "DEVICE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiters = [5e-4, 8e-4, 1e-3, 2e-3, 3e-3, 5e-3, 9e-3, 2e-2, 9e-2]\n",
    "size_label = list(range(CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_to_label(size):\n",
    "    # return the label of the size based on the delimiters\n",
    "    for l, d in zip(CLASSES, delimiters):\n",
    "        if size < d:\n",
    "            return l\n",
    "    return size_label[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_embedding = torch.nn.Embedding(len(size_label), EMBED_DIM).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 3e-3\n",
    "size_class = torch.tensor(size_to_label(size)).to(DEVICE)\n",
    "emb = size_embedding(size_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_embedding.weight.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainable Prompt Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from utils.mobile_sam import sam_model_registry\n",
    "from utils.mobile_sam.predictor import SamPredictor\n",
    "from utils.utils import *\n",
    "from utils.distill_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_checkpoint = \"../bin/mobile_sam.pt\"\n",
    "model = sam_model_registry[\"vit_t\"](checkpoint=sam_checkpoint, add_prompt='random').to('cuda')\n",
    "model.eval()\n",
    "for m in model.image_encoder.modules():\n",
    "    if isinstance(m, torch.nn.BatchNorm2d):\n",
    "        m.eval()\n",
    "        m.weight.requires_grad_(False)\n",
    "        m.bias.requires_grad_(False)\n",
    "student = SamPredictor(model)\n",
    "\n",
    "params = list(student.model.mask_decoder.parameters()) + list(student.model.prompt_encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.model.prompt_encoder.point_embeddings[4].train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.load('../bin/mobile_sam.pt')\n",
    "f = torch.load('../bin/distilled_mobile_sam_prompt_prompt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in enumerate(m.keys()):\n",
    "    if not (m[w].cpu() == f[w].cpu()).all().item():\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['prompt_encoder.point_embeddings.4.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
