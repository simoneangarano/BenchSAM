{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ResizeLongestSide' from 'utils' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/sa58728/BenchSAM/notebooks/Distillation.ipynb Cell 1\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvita10.ece.utexas.edu/home/sa58728/BenchSAM/notebooks/Distillation.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m (SamModel, SamProcessor)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvita10.ece.utexas.edu/home/sa58728/BenchSAM/notebooks/Distillation.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmobile_sam\u001b[39;00m \u001b[39mimport\u001b[39;00m sam_model_registry\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvita10.ece.utexas.edu/home/sa58728/BenchSAM/notebooks/Distillation.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpredictor\u001b[39;00m \u001b[39mimport\u001b[39;00m SamPredictor\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvita10.ece.utexas.edu/home/sa58728/BenchSAM/notebooks/Distillation.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m SA1B_Dataset\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvita10.ece.utexas.edu/home/sa58728/BenchSAM/notebooks/Distillation.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/BenchSAM/notebooks/../utils/predictor.py:14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmobile_sam\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling\u001b[39;00m \u001b[39mimport\u001b[39;00m Sam\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional, Tuple\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m ResizeLongestSide\n\u001b[1;32m     17\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSamPredictor\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     19\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m     20\u001b[0m         sam_model: Sam,\n\u001b[1;32m     21\u001b[0m     ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ResizeLongestSide' from 'utils' (unknown location)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import (SamModel, SamProcessor)\n",
    "from mobile_sam import sam_model_registry\n",
    "from utils.predictor import SamPredictor\n",
    "\n",
    "from datasets import SA1B_Dataset\n",
    "from utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "DATA_DIR = Path('../Datasets/')\n",
    "GPU = 0\n",
    "\n",
    "DEVICE = torch.device(f\"cuda:{GPU}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = SA1B_Dataset(root=DATA_DIR.joinpath('SA_1B/images/'), features=None, split='sa_000009')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, (i, l, n) in enumerate(dataloader):\n",
    "    print(i.shape, l.shape, n)\n",
    "    if j > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(l.unique()))\n",
    "plt.imshow(l[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher\n",
    "teacher = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(DEVICE).eval()\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student\n",
    "model_type = \"vit_t\"\n",
    "sam_checkpoint = \"bin/mobile_sam.pt\"\n",
    "\n",
    "model = sam_model_registry[model_type](checkpoint=None).to(DEVICE).train()\n",
    "student = SamPredictor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = processor(i, input_points=None, return_tensors=\"pt\").to(DEVICE)\n",
    "    t_features = teacher.get_image_embeddings(inputs[\"pixel_values\"]).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse\n",
    "mse = torch.nn.MSELoss()\n",
    "mse(t_features, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller():\n",
    "    def __init__(self, teacher, student, processor, dataloader, optimizer, device):\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "        self.processor = processor\n",
    "        self.dataloader = dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "\n",
    "    def get_distillation_loss(self, img):\n",
    "        student.set_image(img[0].permute((2,0,1)))\n",
    "        s_features = student.features\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = processor(img, input_points=None, return_tensors=\"pt\").to(DEVICE)\n",
    "            t_features = teacher.get_image_embeddings(inputs[\"pixel_values\"])\n",
    "\n",
    "        return torch.nn.functional.mse_loss(s_features, t_features)\n",
    "\n",
    "    def distill(self):\n",
    "        t = tqdm(dataloader, desc='Distillation:')\n",
    "        for img, _, _ in t:\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.get_distillation_loss(img)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            t.set_postfix({'Loss': loss.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(student.model.parameters(), lr=1e-3)\n",
    "distiller = Distiller(teacher, student, processor, dataloader, optimizer, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiller.distill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(distiller.student.model.state_dict(), 'bin/distilled_mobile_sam.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_features_ids = csv.reader(open(Path('results/feature_ids.csv'), 'r'))\n",
    "l = list(teacher_features_ids)\n",
    "#l = [i[0] for i in l]\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.load('results/teacher_features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prompt a random point belonging to an instance\n",
    "- get the corresponding mask and mask size\n",
    "- use saved SAM features\n",
    "- freeze MobileSAM backbone\n",
    "- prompt SAM and MobileSAM and collect output masks (3 masks?)\n",
    "- compute dice and focal loss (20:1)\n",
    "- weight loss based on mask size\n",
    "- OBTAIN LOGITS FROM (MOBILESAM, SAM) !!! (return_logits=True, binarize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_prompt()\n",
    "# get_instance_label()\n",
    "# get_mask_size()\n",
    "# size coefficient = 1 - (mask_size / image_size)\n",
    "# get_output(SAM, saved_features, prompt)\n",
    "# get_output(MobileSAM, saved_features, prompt) \n",
    "# dice_loss()\n",
    "# focal_loss()\n",
    "# loss = (20 * dice_loss() + focal_loss()) * size_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distill import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../Datasets/')\n",
    "SPLIT = 'sa_000020'\n",
    "GPU = 2\n",
    "DEVICE = torch.device(f\"cuda:{GPU}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "SHUFFLE = True\n",
    "LOAD_FEATURES = True\n",
    "FEATURES = 'results/teacher_features.pt' if LOAD_FEATURES else None\n",
    "\n",
    "EPOCHS = 16\n",
    "LR = 1e-3\n",
    "OPTIM = 'adamw'\n",
    "WD = 1e-5\n",
    "LOSS_WEIGHTS = [0,0,1,0] # 20 focal, 1 dice, 0 bce, 0 size\n",
    "\n",
    "MODE = 'decoder' # encoder, decoder, save_features\n",
    "PRETRAINED = True if MODE == 'decoder' else False\n",
    "\n",
    "dataset = SA1B_Dataset(root=DATA_DIR.joinpath('SA_1B/images/'), split=SPLIT,  features=FEATURES, labels=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=SHUFFLE, num_workers=16, pin_memory=True)\n",
    "\n",
    "teacher = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(DEVICE)\n",
    "teacher.eval()\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "\n",
    "model_type = \"vit_t\"\n",
    "sam_checkpoint = \"bin/mobile_sam.pt\" if PRETRAINED else None\n",
    "\n",
    "model = sam_model_registry[model_type](checkpoint=sam_checkpoint).to(DEVICE)\n",
    "model.eval()\n",
    "for m in model.image_encoder.modules():\n",
    "    if isinstance(m, torch.nn.BatchNorm2d):\n",
    "        m.eval()\n",
    "        m.weight.requires_grad_(False)\n",
    "        m.bias.requires_grad_(False)\n",
    "student = SamPredictor(model)\n",
    "\n",
    "if MODE == 'encoder':\n",
    "    DISTILLER = EncDistiller\n",
    "    params = student.model.image_encoder.parameters()\n",
    "else:\n",
    "    DISTILLER = DecDistiller\n",
    "    params = student.model.mask_decoder.parameters()\n",
    "\n",
    "if OPTIM == 'adamw':\n",
    "    optimizer = torch.optim.AdamW(params, lr=LR, weight_decay=WD)\n",
    "elif OPTIM == 'adam':\n",
    "    optimizer = torch.optim.Adam(params, lr=LR)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "distiller = DISTILLER(teacher, student, processor, dataloader, optimizer, scheduler, loss_weights=LOSS_WEIGHTS, device=DEVICE)\n",
    "\n",
    "if MODE == 'save_features':\n",
    "    distiller.save_teacher_features(Path('results/teacher_features.pt'))\n",
    "else:\n",
    "    distiller.distill(epochs=EPOCHS, accumulate=BATCH_SIZE, use_saved_features=LOAD_FEATURES, name=MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tianrun-chen/SAM-Adapter-PyTorch/blob/60bd2770b1c4fcb38d98822cae9a17781601ff9e/models/mmseg/models/sam/image_encoder.py#L263"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
